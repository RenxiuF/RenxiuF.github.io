---
title: LlamaIndex及魔搭平台API-Inference服务的使用
filename: 20250907  
tags:  
  - blog   
  - rag  
categories:  
  - 大模型应用开发
date: 2025-09-07  
description: LlamaIndex  
articleGPT: 这篇介绍了 AI Agent 框架 LlamaIndex 的基本用法并提供了相关资料
top: true  
share: true  
delete: false  
---

这里仅记录相关功能的调用方式以及常用的资料，不作详细介绍。

[TOC]

## LlamaIndex 简介
LlamaIndex是一个完整的工具包，用索引和工作量创建LLM驱动的Agent。主要有三个重要的功能：
- Components
- Agents and Tools
- Workflows

## 库的下载
下载相关功能使用下面的组合方式
```bash
pip install llama-index-{component-type}-{framework-name}
```
比如 llm、 embeddings 等功能库的下载
```bash
pip install llama-index-llms-huggingface-api
pip install llama-index-embeddings-huggingface
```

## API 文档
[API Reference](https://docs.llamaindex.ai/en/stable/api_reference/)

## 使用 ModelScope 平台的 API-inference
魔搭平台的[API-Inference](https://www.modelscope.cn/docs/model-service/API-Inference/intro)，针对大语言模型提供OpenAI API兼容的接口，根据提供的[调用方式](https://docs.llamaindex.ai/en/stable/api_reference/llms/openai_like/)。
```bash
pip install llama-index-llms-openai-like
```
```python
from llama_index.llms.openai_like import OpenAILike
import os

os.environ["ModelScope_API_KEY"] = "ms-8d..."

llm = OpenAILike(
    model="Qwen/Qwen2.5-Coder-32B-Instruct",
    api_base="https://api-inference.modelscope.cn/v1",
    api_key=os.environ["ModelScope_API_KEY"],
    context_window=128000,
    is_chat_model=True,
    is_function_calling_model=False,
)
response = llm.complete("Hello, how are you?")
print(response)
```
## 工作流-手动
定义一个继承自`Workflow`的类，用`@step`创建单步工作流，添加`StartEvent`和`StopEvent`，指示工作流开始和结束

```bash
pip install llama-index-core
```

```python
from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step
from llama_index.core.workflow import Event, Context
import random

class ProcessingEvent(Event):
  intermediate_result: str

class LoopEvent(Event):
  loop_output: str

class MultiStepWorkflow(Workflow):
  @step
  async def step_one(self, ctx: Context, ev: StartEvent | LoopEvent) -> LoopEvent | ProcessingEvent:
    if random.randint(0, 1) == 0:
      print("it is 0")
      return LoopEvent(loop_output="Back to step one.")
    else:
      print("it is 1! Go to next one.")
      # 设置要传递的信息
      await ctx.store.set("query", "Hello, who are you?")
      return ProcessingEvent(intermediate_result="First step complete.")

  @step
  async def step_two(self, ctx: Context, ev: ProcessingEvent) -> StopEvent:
    query = await ctx.store.get("query")
    print(f"query: {query}")
    final_result = f"finished process: {ev.intermediate_result}"

    return StopEvent(result=final_result)

w = MultiStepWorkflow(timeout=10, verbose=False)
result = await w.run()
result
```
`Workflow`可以在step之间传递信息

## 工作流-自动化
`AgentWorkflow`

可以用`initial_state`传递初始状态信息，所有Agent都可以使用

```bash
pip install llama-index-llms-openai-like
```
```python
from llama_index.llms.openai_like import OpenAILike
from llama_index.core.agent.workflow import AgentWorkflow
from llama_index.core.agent import ReActAgent
from llama_index.core.workflow import Context
import os

os.environ["ModelScope_API_KEY"] = "ms-..."
# 调用魔搭平台的API
llm = OpenAILike(
    model="Qwen/Qwen2.5-Coder-32B-Instruct",
    api_base="https://api-inference.modelscope.cn/v1",
    api_key=os.environ["ModelScope_API_KEY"],
    context_window=128000,
    is_chat_model=True,
    is_function_calling_model=False,
)

async def add(ctx: Context, a: int, b: int) -> int:
  """add two integer numbers"""
  # 获取信息
  cur_state = await ctx.store.get("state")
  cur_state["num_fn_calls"] += 1
  await ctx.store.set("state", cur_state)

  return a + b

async def multiply(ctx: Context, a: int, b: int) -> int:
  """multiply two integer numbers"""
  # 获取信息
  cur_state = await ctx.store.get("state")
  cur_state["num_fn_calls"] += 1
  await ctx.store.set("state", cur_state)
  return a * b


add_agent = ReActAgent(
    name="add_agent",
    description="is able to add two integer number.",
    tools=[add],
    llm=llm
)

multi_agent = ReActAgent(
    name="multi_agent",
    description="is able to multiply two integer numbers.",
    tools=[multiply],
    llm=llm
)


w = AgentWorkflow(
    agents=[add_agent, multi_agent],
    root_agent="add_agent",
    initial_state={"num_fn_calls": 0},
    state_prompt="Current state: {state}. User message: {msg}",
)

ctx = Context(w)
result = await w.run("Can you multi 3 and 5", ctx=ctx)
print(f"{result.response.blocks[0].text=}")

state = await ctx.store.get("state")
print(f"========\n{state['num_fn_calls']}")



```

